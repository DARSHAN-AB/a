pg:1

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = sns.load_dataset("iris")

print("\n--- First 5 rows ---")
print(df.head())
print("\n--- Data Types &amp; Missing Values ---")
print(df.info())
print("\nMissing values:\n", df.isnull().sum())
print("\n--- Summary Statistics ---")
print(df.describe(include='all'))
print("\n--- Class Distribution ---")
print(df['species'].value_counts())

df.hist(figsize=(8,6))
plt.suptitle("Histograms of Iris Features", fontsize=14)
plt.show()

plt.figure(figsize=(8,6))
sns.boxplot(data=df, orient="h")
plt.title("Boxplots of Iris Features")
plt.show()

sns.pairplot(df, hue="species", diag_kind="hist")
plt.suptitle("Pairwise Scatter Plots of Iris Features", y=1.02)
plt.show()

plt.figure(figsize=(7,5))
sns.scatterplot(x="sepal_length", y="sepal_width", hue="species", data=df, s=70)
plt.title("Sepal Length vs Sepal Width")
plt.show()

=========================================================================================

pg:2

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

url ="https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv"
df = pd.read_csv(url)
print("\n--- First 5 rows ---")
print(df.head())
print("\n--- Data Info ---")
print(df.info())
print("\n--- Summary Statistics ---")
print(df.describe())

X = df[['rm']]
y = df['medv']

model = LinearRegression()
model.fit(X, y)
print("\nIntercept (b0):", model.intercept_)
print("Slope (b1):", model.coef_[0])

y_pred = model.predict(X)

print("\nMean Squared Error:", mean_squared_error(y, y_pred))
print("R² Score:", r2_score(y, y_pred))

plt.figure(figsize=(8,6))
plt.scatter(X, y, color='blue', alpha=0.6, label='Actual')
plt.plot(X, y_pred, color='red', linewidth=2, label='Regression line')
plt.xlabel("Average number of rooms per dwelling (RM)")
plt.ylabel("Median home value (MEDV)")
plt.title("Simple Linear Regression: RM vs MEDV")
plt.legend()
plt.show()

residuals = y - y_pred
plt.figure(figsize=(8,6))
plt.scatter(y_pred, residuals, color='purple', alpha=0.6)
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel("Predicted MEDV")
plt.ylabel("Residuals (Actual - Predicted)")
plt.title("Residuals Plot")
plt.show()

======================================================================================

pg:3

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report

data = load_breast_cancer()
X = data.data
y = data.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)

model = LogisticRegression(max_iter=10000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Logistic Regression Model Evaluation:")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")

print(f"Recall:    {recall:.4f}")
print("\nConfusion Matrix:")
print(conf_matrix)
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=data.target_names))

plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

==========================================================================================

pg:4

import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from matplotlib.colors import ListedColormap

iris = datasets.load_iris()
X = iris.data[:, :2]
y = iris.target

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

def plot_decision_boundary(k):
    clf = KNeighborsClassifier(n_neighbors=k)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    print(f"k = {k}: Accuracy = {acc:.2f}")
    
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),np.arange(y_min, y_max, 0.02))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
    cmap_bold = ['red', 'green', 'blue']
    plt.figure(figsize=(6, 4))
    plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)
    for i, color in zip(range(len(iris.target_names)), cmap_bold):
        plt.scatter(X[y == i, 0], X[y == i, 1],c=color, label=iris.target_names[i], edgecolor='k', s=40)
        plt.scatter(X_test[:, 0], X_test[:, 1], c='yellow', edgecolor='k',marker='*', s=120, label='Test points')
        plt.title(f"k-NN Decision Boundary (k = {k})")
        plt.xlabel('Sepal length (cm)')
        plt.ylabel('Sepal width (cm)')
        plt.legend()
    plt.show()

for k in [1, 3, 5, 7, 11]:
    plot_decision_boundary(k)

========================================================================================================================

pg:5

import pandas as pd
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import matplotlib.pyplot as plt

iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='target')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

print("Decision Tree Classifier Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred,
target_names=iris.target_names))

plt.figure(figsize=(12, 8))
plot_tree(model,
    feature_names=iris.feature_names,
    class_names=iris.target_names,
    filled=True,
    rounded=True,
    fontsize=10)
plt.title("Decision Tree Visualization - Iris Dataset")
plt.show()

print("\nDecision Rules:\n")
rules = export_text(model, feature_names=iris.feature_names)
print(rules)

===================================================================================================

pg:6

import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler

iris = datasets.load_iris()
X = iris.data

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X_scaled)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

plt.figure(figsize=(8, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', s=50)
plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, marker='X', label='Centroids')
plt.title("K-Means Clustering on Iris Dataset")
plt.xlabel("Feature 1 (standardized)")
plt.ylabel("Feature 2 (standardized)")
plt.legend()
plt.show()

print("Cluster Centers (Standardized Feature Space):\n", centroids)

y_true = iris.target
print("\nActual Labels (0=setosa, 1=versicolor, 2=virginica):")

print(y_true[:10])
print("Predicted Cluster Labels:")
print(labels[:10])

=================================================================================================

pg:7

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt

digits = datasets.load_digits()

print("Image Data Shape:", digits.data.shape)
print("Label Data Shape:", digits.target.shape)

plt.figure(figsize=(8, 4))
for index, (image, label) in enumerate(zip(digits.data[0:8], digits.target[0:8])):
    plt.subplot(2, 4, index + 1)
    plt.imshow(image.reshape(8, 8), cmap=plt.cm.gray)
    plt.title(f'Target: {label}')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.3, random_state=42)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

svm_clf = SVC(kernel='rbf', gamma=0.05, C=10)
svm_clf.fit(X_train, y_train)

y_pred = svm_clf.predict(X_test)


print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

plt.figure(figsize=(8, 4))
for i in range(8):
    plt.subplot(2, 4, i + 1)
    plt.imshow(X_test[i].reshape(8, 8), cmap=plt.cm.gray)
    plt.title(f'Pred: {y_pred[i]} | True: {y_test[i]}')
plt.show()

==========================================================================================================================

pg:8

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

print("Loading MNIST dataset...")
mnist = fetch_openml('mnist_784', version=1)
X = mnist.data
y = mnist.target.astype(int)
print("Dataset shape:", X.shape)

print("Standardizing data...")
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=50)
X_pca = pca.fit_transform(X_scaled)
print("Reduced dataset shape:", X_pca.shape)

plt.figure(figsize=(8, 5))
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o')
plt.title('Explained Variance by Principal Components')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance')

plt.grid(True)
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:10000, 0], X_pca[:10000, 1], c=y[:10000], cmap='tab10', s=10)
plt.colorbar(label='Digit Label')
plt.title('MNIST data projected onto first 2 Principal Components')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()
